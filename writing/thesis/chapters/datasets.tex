\chapter{Datasets}\label{cha:datasets}
We use different datasets in this thesis.
In this chapter, we look at all used datasets and document the preprocessing
steps.

\todo{Insert correct values into dataset tables.}
\todo{Use tabular numbers instead of old-style figures here!}
We use the following datasets:

\begin{table}[h]
\begin{tabular}[c]{lrr}
  \toprule \multicolumn{1}{r}{\textbf{Name}}
& \multicolumn{1}{r}{\textbf{No.~of predictors}}
& \multicolumn{1}{r}{\textbf{No.~of instances}}
\\\midrule
  Friedman1 & 10 & 10000 \\
  Concrete & 9 & 1030 \\
  Power Plant & 4 & 9568 \\
  Friedman9 & 4  & 200 \\
  Friedman9 & 4  & 200
\\\bottomrule
\end{tabular}
\caption[List of Datasets]{Datasets used for this thesis.
  Included is the number of predictors(i.e.~no.~of dimensions) and the number of provided training examples.}
\end{table}

Each feature of each dataset was scaled to the range \([0,1]\) with the so called min-max
scaler
\begin{equation*}
  \operatorname{scale}(\bm{x}) = \frac{\bm{x} - \min({\bm{x}})}{\max{(\bm{x}) - \min(\bm{x})}}.
\end{equation*}

Because some features of some datasets are distributed in a non-optimal way, we perform a Box--Cox transformation for them using a parameter \(\lambda\):
\begin{equation*}
  \operatorname{Box--Cox}(x_i) =
  \begin{cases}
    \ln(x_i) & \text{for } \lambda = 0, \\
    x_i^\lambda - 1 & \text{otherwise}.
  \end{cases}
\end{equation*}
We estimate the choice for \(\lambda\) that results in the closest fit to a
normal distribution using the optimization procedure contained in the statistics module of Scipy.
Note that we need to shift the data before applying this transformation by a small positive number, because \(ln(0)\) is undefined.
The used parameters for each dataset can be found in the following tables.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
