\chapter{Introduction}\label{chapter:introduction}
We live in a world that runs on data.
Machine learning and statistics are the fuel, that continues to power the
ever-growing service industry but also supply companies with needed business
intelligence.

Throughout this thesis we will discuss supervised learning methods that are
based on the sparse grids methodology.
Sparse grids is a family of closely-related discretization techniques that
originates from numerical partial differential equations.
It approximates functions with many simple basis functions.
Because of its origins, the notation and terminology used is different from the
standard machine learning one.
We try to fit the sparse grid learning methods into a coherent and simple
statistical framework that is natural to the application.

The goal of this thesis is to introduce and evaluate different ways to integrate
prior information into our data mining procedure.
In this context prior information corresponds to something akin to the standard
Bayesian prior.
This prior represents assumptions, which can be either drawn from the dataset or
from the inherent properties of the sparse grid methods.

The thesis starts with a chapter explaining the needed preliminary mathematical
techniques and especially the fundamentals of the sparse grid framework.

We then look at different ways of including prior information into this learning method.
We make the following contributions:
\begin{itemize}
\item In \cref{cha:regularization} we evaluate different regularization
  methods that help us to impose smoothness constrains on our model and allows us
  to simplify our models.
  We begin with a discussion of regularization theory and then segue into an
  analysis of two fundamental methods: Tikhonov regularization and
  sparsity-inducing penalties.
  The regularization methods represent different assumptions.
  We present both methods that show promising results under only mild conditions
  and methods, which are better suited for problems where we can make stronger
  assumptions about the data.
  The chapter also contains a discussion of an alternative state-of-the-art solver for regularized linear systems, that is able to handle the newly added methods.
\item In \cref{cha:grid-gen} we evaluate different grid construction methods.
  In contrast to the chapter before, we use our prior knowledge to modify the
  grid generation process, which is more efficient than the regularization
  approach, but also requires stronger assumptions than the basic regularization approaches.
  We first discuss a generalized form of sparse grids, that allows us to modify
  the granularity of the generated grid.
  Secondly we introduce interaction-term-aware sparse grids that allow us to construct grids with a different granularity depending on the dimensions and their interaction with each other.
  The methods discussed in that chapter allow us to tackle very-high dimensional problems, that are impossible or inefficient for regular sparse grids.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
