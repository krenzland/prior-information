% !TeX root = ../main.tex
\chapter{Regularization Methods}
During the discussion of \autoref{eq:optGoal} we have neglected the regularization
operator.

In this chapter we are going to compare multiple choices and evaluate their effectiveness.

\section{Tikhonov Regularization}
We define Tikhonov regularization as 
\begin{equation}
\mathcal{S} = \Vert \Gamma \bm{\alpha} \Vert_2^2.
\end{equation}
In this equation \(\Gamma\) is a linear operator
and \(\Vert x \Vert_2^2 = \sum_x x^2\) is the L2 vector norm.

A common choice for \(\Gamma\) is the identity matrix as proposed in \cite{SpatAdaptGrid}.
From a Bayesian perspective this construction resembles a Gaussian prior on the weights with mean 0 and
a constant variance and is therefore distributed as follows
\begin{equation}
\alpha \sim \mathcal{N} (0, \bm{I} \sigma^2 ).
\end{equation}
It is clear that we assume that all weights are distributed with the same variance. 

We construct an alternative matrix as follows:
\begin{align}
  \label{eq:diagonalMatrix}
k & = \vert l \vert_1 - d + 1 \\
\bm{\Gamma}_{i,i} & = \vert ^1_4\vert^{k-1}.
\end{align}
This construction originates from ???.

\section{Lasso and Elastic net}
We can use lasso regression to replace our regularisation operator with 
\begin{equation}
\mathcal{S} = \Vert \bm{\alpha} \Vert_1,
\end{equation}
which uses the Manhattan norm defined by \(\Vert \bm{x} \Vert_1 = \sum_i \vert x_i \vert\)

Lasso regression can reduce some weights to zero. 
This results in an implicit feature selection and thus leads to sparsity.

Because the absolute value function is not differentiable at \(0\), we cannot use
optimization methods which rely on the existence of the gradient such as
conjugated gradients.
We must use a proximal gradient method, for example FISTA.

\input{chapters/regularization_fista}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:

